{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cb0d9b2-f626-442d-b95a-670efb958c93",
   "metadata": {},
   "source": [
    "# Meeting Assistant using Whisper & Llama3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06601e56-bfdd-4421-849a-790fb44e1ebf",
   "metadata": {},
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "069bb4ae-995a-4834-ba37-e83f16c49895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "import torch\n",
    "import transformers\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from huggingface_hub import login\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, TextStreamer\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ac05b3b-3edd-459d-a41f-b4e014ba85c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8f4dd48-4421-423f-b638-261d26e7ba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio file\n",
    "ABSOLUTE_PATH = os.path.abspath(os.getcwd())\n",
    "AUDIO_DIR = \"audio\"\n",
    "RECORDING_NAME = \"user_audio.mp3\"\n",
    "\n",
    "RECORDING_PATH = os.path.join(ABSOLUTE_PATH, AUDIO_DIR)\n",
    "RECORDING_FILE = os.path.join(RECORDING_PATH, RECORDING_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ad3b9ea-69ab-4511-8739-99b45e6efbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "# Get OpenAI creds\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"type-your-api-key-here\")\n",
    "openai_client = OpenAI()\n",
    "\n",
    "# Login Hugging Face\n",
    "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\", \"type-your-token-here\")\n",
    "login(HUGGINGFACE_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1cb79d8-5eff-46cd-95f9-022a8c232a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "AUDIO_MODEL = \"whisper-1\"\n",
    "LLAMA = \"meta-llama/Llama-3.2-3B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db96fb31-1f72-4d72-a0a3-2b0a4ca34308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd9a8b58d89462cadf386919b5e1436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Llama model with quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "# Load Llama tokenizer\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "\n",
    "streamer = TextStreamer(llama_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e133423-439b-4f37-818a-01d22e6c0f91",
   "metadata": {},
   "source": [
    "# Speech Recognition and Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10ab557e-6b4a-4ad6-a0d2-6356d8bd4c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio() -> str:\n",
    "    # Load recording file\n",
    "    recording_file = open(RECORDING_FILE, \"rb\")\n",
    "    \n",
    "    # Speech Recognition and Transcription\n",
    "    transcription = openai_client.audio.transcriptions.create(\n",
    "        model=AUDIO_MODEL,\n",
    "        file=recording_file,\n",
    "        response_format=\"text\"\n",
    "    )\n",
    "\n",
    "    return transcription\n",
    "\n",
    "# Testing\n",
    "# transcribe_audio()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9597d7e-9df6-476a-8eb9-5003e466ddee",
   "metadata": {},
   "source": [
    "# Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36e0b8cc-dd24-40f0-a29e-2495bea036c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a meeting assistant that generates concise, \\\n",
    "well-structured meeting minutes in markdown from transcripts. \\\n",
    "Include: summary, key discussion points, takeaways, and action items with owners.\"\n",
    "\n",
    "user_prompt = f\"Given the transcript below, generate meeting minutes in markdown with: \\\n",
    "summary, key discussion points, takeaways, and action items with owners. \\\n",
    "Transcript:\\n\"\n",
    "\n",
    "def generate_messages(\n",
    "    transcription: str,\n",
    "    user_prompt: str=user_prompt,\n",
    "    system_prompt: str=system_prompt\n",
    ") -> list:\n",
    "    user_prompt += transcription\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27413515-5a54-4ea1-870e-56a5ce0d6cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(decoded: str) -> str:\n",
    "    \"\"\"Clean special tokens from the generated text\"\"\"\n",
    "    if \"<|start_header_id|>assistant<|end_header_id|>\" in decoded:\n",
    "        assistant_part = decoded.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1] # get assistant part only\n",
    "        assistant_reply = assistant_part.split(\"<|eot_id|>\")[0].strip()\n",
    "    else:\n",
    "        assistant_reply = decoded.strip()  # fallback, if format changes\n",
    "    return assistant_reply\n",
    "\n",
    "def generate_mom(transcription: str):\n",
    "    # Compile messages\n",
    "    messages = generate_messages(transcription)\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = llama_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    # Inference\n",
    "    outputs = llama_model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens=2000\n",
    "    )\n",
    "    \n",
    "    # Extract response\n",
    "    decoded = llama_tokenizer.decode(outputs[0])\n",
    "    response = extract_text(decoded)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d84a89cc-443f-400b-9f3b-7cc4054716fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "# mom = generate_mom(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac58f91d-2f8e-4fab-89fe-08629ed7c270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_audio(file_path):\n",
    "    if not file_path or not os.path.exists(file_path):\n",
    "        raise ValueError(\"No audio file uploaded or file does not exist.\")\n",
    "        \n",
    "    try:\n",
    "        shutil.move(file_path, RECORDING_FILE)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to save audio: {str(e)}\")\n",
    "        \n",
    "def process(file_path):\n",
    "    try:\n",
    "        # Save uploaded audio file\n",
    "        save_audio(file_path)\n",
    "\n",
    "        # Transcribe audio\n",
    "        transcription = transcribe_audio()\n",
    "        \n",
    "        # Generate Minutes of Meeting\n",
    "        mom = generate_mom(transcription)\n",
    "        return mom\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"**Error:** {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a66dd7f-ce12-478a-8d51-2cdd7fc8e638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        gr.Markdown(\"# Your Meeting Assistant\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            audio_input = gr.Audio(type=\"filepath\")\n",
    "            submit_btn = gr.Button(\"Generate MoM\")\n",
    "        output_md = gr.Markdown()\n",
    "        \n",
    "    submit_btn.click(\n",
    "        fn=process,\n",
    "        inputs=audio_input,\n",
    "        outputs=output_md\n",
    "    )\n",
    "    \n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf24b8a-9f34-44fe-b744-e84dc0fb74e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugface",
   "language": "python",
   "name": "hugface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
